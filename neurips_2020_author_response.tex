\documentclass{article}

\usepackage{neurips_2020_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{lipsum}

\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{blkarray}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{cleveref}

\input{mathdef.tex}

\begin{document}
	
We thank all reviewers for their careful reading, their insightful and constructive comments, and for recognizing the importance of this research area (\hlr{R1},\hlg{R2},\hlb{R3},\hlm{R4}), the effort we put in bringing together tools from different areas (\hlr{R1},\hlg{R2},\hlb{R3},\hlm{R4}), and the soundness of theoretical results (\hlr{R1},\hlm{R4}). We answer the main concerns below, but will incorporate all feedback (typos, presentation, additional references) in the final version.

\paragraph{(\hlg{R2}) Unclear connections between the finite planning and the infinite horizon performance $V$.} We decided to study separately the errors stemming from the approximation of $V^r$ by $\hat{V}^r$ for any controls $\mathbf{u}$ (in Thm. 3); and those caused by the optimization of $\hat{V}^r$ with a finite budget $K$ (in Thm. 2). Following your suggestion, we propose to reformulate Thm. 3 to also account for the effect of finite planning, through the planned action $a_K$ (see next point).

\paragraph{(\hlg{R2}) Reliance of the main result on the PE assumption.} We resorted to this assumption to get asymptotic near-optimal performance when $N\to\infty$. It is quite common in the control literature, as recognized by \hlm{R4}. % and was used in similar context (LQ control) by [11,2]. 
However, as you pointed out, our non-asymptotic analysis actually yields a stronger input-dependent result. Accordingly, we propose to remove the PE assumption from Thm. 3, which reverts the $\log N / N$ term in the bound back to the input-dependent term $\beta_N(\delta)^2/\lambda_{\min}(G_{N,\lambda})$. Thus, our main result becomes:
\begin{equation*}
\hat{V}^r(a_K) \leq {V}^r(a^\star) \leq V(a^\star) \leq \hat{V}^r(a_K) + \hlr{\underbrace{\Delta_\omega}_{\substack{\text{robustness to}\\ \text{perturbations}}}} + \hlb{\underbrace{\cO\left(\frac{\beta_N(\delta)^2}{\lambda_{\min}(G_{N,\lambda})}\right)}_{\text{estimation error}}} + \hlg{\underbrace{\cO\left(K^{-\frac{\log 1/\gamma}{\log \kappa}}\right)}_{\text{planning error}}}
\end{equation*}
 Then, we will mention in a corollary that under the (restrictive) PE assumption, this estimation error term reduces to the more explicit form $\hlb{{\cO\left(\frac{\log(N^{d/2}/\delta)}{N}\right)}}$ which ensures asymptotic near-optimality when $N\to\infty$ (and $K\to\infty$).

\paragraph{(\hlg{R2}) Unclear connection between the discrete measurement noise and the continuous time noise process.}  Indeed, the reasoning of the remark on line 135 is insufficient, since it only bounds $\omega(t)$ at discrete time steps $(t_n)_{n\in\Natural}$ while the interval prediction requires bounds $\underline{\omega}(t)\leq\omega(t)\leq\overline{\omega}(t)$ that hold at all times $t\geq 0$. We overlooked it, and are very grateful to \hlg{R2} for pointing it out. Thankfully, this gap can easily be fixed. The most straightforward way is to directly assume bounded perturbations, such a hypothesis is both realistic and ubiquitous in the robust MPC literature [17, 4, 6, 28, 22, 29]. Another solution, in the spirit of our remark, is to further specify the continuous-time noise process $\omega(t)$: in place of Assumpt. 2, the disturbance $\omega(t)$ can be modeled as a Wiener process $W_t$. This has two effects: first, the increments $W_{t+\dd t} - W_t$ are independently and normally distributed, which allows keeping the estimation procedure of Thm. 1 by simply replacing $y_n$ and $\Phi_n$ by the differences $\tilde{y}_n = y_n-y_{n-1}$ and $\tilde{\Phi}_n = \Phi_n-\Phi_{n-1}$ in the expressions of $\theta_{N,\lambda}$ and $G_{N,\lambda}$. Second, the running maximum $\max_{0\leq s\leq t} W_s$ of a Wiener process is Half-normally distributed, which allows to define bounds $\overline{\omega}_n$ on $W_t$ that hold with probability $\delta_n$ \emph{for all $0\leq t\leq t_n$}. Then, a union bound over all $[0,t_{n}]$ intervals can be applied to effectively bound $\omega(t)$ for all $t\geq 0$. That being said, we would rather include the first solution (bounded perturbations assumption) in the final version for simplicity.

\paragraph{(\hlg{R2}) Suboptimality result defined as regret.} The term \textit{simple} (as opposed to cumulative) \textit{regret} has been used in the planning literature [e.g. 19] as a synonym for \textit{suboptimality} . We agree to use the latter, less ambiguous, instead.

\paragraph{(\hlg{R2},\hlb{R3}) Relevance of the multi-model extension.}
We added this section to mitigate the assumption of linear dynamics and uncertainty, which \hlb{R3} finds quite restrictive. Allowing multiple models enables to make multi-modal predictions, which we consider a substantial improvement: it was \textit{e.g.} necessary for our simulated driving experiment.

\paragraph{(\hlb{R3}) The discretization of controls might result in some loss of performance.}
Indeed. However, as suggested in line 67, this issue could be circumvented by resorting to more complex planning algorithms for continuous actions, such as \texttt{SOPC} [10] which also comes with a regret bound. We settled for discrete actions for ease of presentation.

\paragraph{(\hlm{R4}) Comparison against other methods.}
We have wondered about what methods to compare against. In our related work, we made the case that three major families of robust control (robust stabilization, robust constraint satisfaction, and minimax LQ) cannot be applied to our environments, which are not stabilization tasks, involve non-convex constraints and cannot be solved with quadratic costs. Turning to non-robust RL methods, we argued in lines 249-252 that a model-based approach with the same dynamics priors and planning algorithm (our \textit{nominal} baseline) would make the fairest comparison. We have since run an additional comparison to DQN, which converged in 30k samples --causing 2k collisions-- to a policy that still suffers a 6\% collision rate. This discussion will be included in the final version. 
% We may add a comparison to [Constrained Policy Optimization, Achiam et al, 2017], in a different setting.

\paragraph{(\hlr{R1}) More varied environments.} We share your enthusiasm about trying more applications. Note however that the driving experiment with a state space of dimension $44$ is already quite ambitious compared to usual experimental settings in similar works, often limited to systems of dimension 2, 3 or 4 [3,4,11,12,17,28,29,34,38].

\end{document}