\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{caption}
\usepackage{blkarray}
\usepackage{todonotes}
\usepackage{mathtools}
\usepackage[backend=biber,style=authoryear,natbib=true,url=false]{biblatex}

\addbibresource{references.bib}

\input{mathdef.tex}

\title{Robust Estimation, Prediction and Control with Dynamics Structure}
\author{Edouard Leurent, Denis Efimov, Odalric-Ambrym Maillard}
\date{November 2019}

\begin{document}

\begin{assumption}[Noise Model]
\label{assumpt:noise}
At each time $t\geq0$ the measurement noise $\eta(t)$ and disturbance $\omega(t)$  are independent Gaussians with respective means $0_r, 0_s$ and covariances $\Sigma_r, \Sigma_s$:
\begin{equation*}
    \eta(t)\sim \cN(0_r,\Sigma_r),\quad\omega(t)\sim \cN(0_s,\Sigma_s)
\end{equation*}
We further require that the matrix $\Sigma_p = C\Sigma_r C^T + D\Sigma_s D^T \in \Real^{p\times p}$ is full-rank.
\end{assumption}

\begin{proposition}[Maximum Likelihood Estimate]
\label{prop:mle}
The maximum likelihood estimate for $\theta$ minimises a weighted mean square error:
\begin{equation*}
    \argmax_{\theta\in\Real^d} \probability{\cD\condbar\eqref{eq:dynamics};\Sigma} = \argmin_{\theta\in\Real^d} \sum_{n=1}^N \|y_n -\theta^T\phi x_n\|_{\Sigma_p^{-1}}^2
\end{equation*}

By rewriting $\Phi_n = [\cdots \phi_i x_n \cdots]\in\Real^{p\times d}$,
\begin{equation*}
    \argmax_{\theta\in\Real^d} \probability{\cD\condbar\eqref{eq:dynamics};\Sigma} = \argmin_{\theta\in\Real^d} \sum_{n=1}^N \|y_n -\Phi_n \theta\|_{\Sigma_p^{-1}}^2
\end{equation*}
\end{proposition}

\subsection{Scalar formulation}

We stack the observations $y_n, x_n$ in $\cD$, as $Y_{[N]}, X_{[N]}\in\Real^{N\times p}$ and the noises $C\eta(t_n)+D\omega(t_n)$ as $\Omega_{[N]} \in\Real^{N\times p}$ to obtain the regression problem:

\begin{equation}
    \label{eq:regression}
    Y_{[N]} = \Phi_{[N]} \theta + \Omega_{[N]},
\end{equation}
with $\Phi_{[N]} = X_{[N]}\phi^T\in \Real^{N\times p\times d}$.


\begin{proposition}[Decoupling the noise]
\label{prop:decoupling}
Without loss of generality, up to a change of basis, we can assume that $\Sigma_d$ is diagonal.
\end{proposition}

In order to solve the regression problem \eqref{eq:regression} can simply reshape the two first $N\times p$ dimensions into a single $Np$ dimension. Since $\Sigma_p$ is diagonal, the noises on each row of $\Omega$ are independent, the problem becomes a standard scalar regression problem with heteroscedastic noise (if $\Sigma_p$ is not proportional to $I_p$), and the Lemma 5 of \citet{kirschner18heteroscedastic} can be applied.

\begin{lemma}[Confidence ellipsoid \citep{kirschner18heteroscedastic}]
Assuming that $\Sigma_p$ is diagonal and $\|\theta\|_2\leq S$, it holds with probability $1-\delta$ that:
\begin{equation}
    \label{eq:confidence-ellipsoid}
    \|\theta - \theta_{Np,\lambda}\|_{G_{Np,\lambda}} \leq \sqrt{2\log \frac{\text{det}(G_{Np,\lambda})^{1/2}\text{det}(\lambda I)^{-1/2}}{\delta}} + \lambda^{1/2}S
\end{equation}
where
\begin{align*}
    \Phi_{[Np]}& \in\Real^{Np\times d},\, Y_{[Np]}\in \Real^{Np\times 1} \text{ are the reshaped versions of $\Phi_{[N]}$, $Y_{[N]}$};\\
    \tilde{\Phi}_{[Np]} &= [\underbrace{\cdots \Sigma_p^{-\frac{1}{2}} \cdots}_{\text{$N$ times}}]^T \Phi_{[Np]} , \qquad  \tilde{Y}_{[Np]} = [\underbrace{\cdots \Sigma_p^{-\frac{1}{2}} \cdots}_{\text{$N$ times}}]^TY_{[Np]};\\
    G_{Np, \lambda} &= \tilde{\Phi}^T_{[Np]}\tilde{\Phi}_{[Np]} + \lambda I_d;\\
    \theta_{Np,\lambda} &= G_{Np, \lambda}^{-1} \tilde{\Phi}^T_{[Np]}\tilde{Y}_{[Np]}.
\end{align*}
\end{lemma}



\section{Proofs}

\subsection{Proof of \autoref{prop:mle}}

\begin{proof}
For $n\in[N]$, at time $t_n$, we have
\begin{align*}
    \probability{y_n, x_n} 
    &= \probability{y_n|x_n}\probability{x_n}\\
    &= \probability{y_n = \dot{x}_n - Bu_n +C\eta_n|x_n,u_n}\probability{x_n}\\
    &= \probability{A(\theta)x_n + B u_n + D \omega_n - Bu_n +C\eta_n = y_n|x_n}\probability{x_n}\\
    &= \probability{C\eta_n + D \omega_n = y_n- \theta^T \phi x_n |x_n}\probability{x_n}\\
    &= \cN(y_n - \theta^T \phi x_n; 0_p,\Sigma_p)\probability{x_n}
\end{align*}
By independence of the noise at different times, we have:
\begin{align*}
    \log\probability{\cD;\Sigma} &= \log\prod_{n=1}^N\probability{y_n, x_n}\\
    &= \sum_{n=1}^N\log \cN(y_n- \theta^T \phi x_n;0_p,\Sigma_p) + \log\probability{x_n}\\
    &= \sum_{n=1}^N -\frac{1}{2} \|y_n -\theta^T\phi x_n\|_{\Sigma_p^{-1}}^2 - \log\left((2\pi)^{p/2} \det(\Sigma_p)^{1/2}\right) + \log\probability{x_n}
\end{align*}
\end{proof}

\subsection{Proof of \autoref{prop:decoupling}}

\begin{proof}
Since $\Sigma_p$ is a symmetric real matrix, it is diagonalisable: there exists $Q\in\cO(p)$ and a diagonal matrix $R$ such that $\Sigma_p = QRQ^{-1}$. Hence, from \eqref{eq:dynamics} and \eqref{eq:measurement}:
\begin{align*}
    Q^{-1}y'(t) &= \sum_{i=1}^d \theta_i Q^{-1}\phi_i x(t) + Q^{-1}(C\eta(t) + D\omega(t))
\end{align*}
By rewriting $y \leftarrow Q^{-1}y, \phi_i\leftarrow Q^{-1}\phi_i, C\leftarrow Q^{-1}C$, and $D\leftarrow Q^{-1}D$ we recover \eqref{eq:dynamics} and \eqref{eq:measurement} in their original form, while now having $C\Sigma_rC^T + D\Sigma_rD^T = R$.
\end{proof}

\end{document}
