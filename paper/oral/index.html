<!doctype html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Safe and Efficient Reinforcement Learning for Behavioural Planning in Autonomous Driving</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/black.css" id="theme">
	<link rel="stylesheet" href="css/custom.css">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">


</head>
<body>
	<div class="reveal">
		<div class="slides">
			<section class="title">
				<h3>Robust-Adaptive Control<br>of Linear Systems:</h3>
				<h4>beyond Quadratic Costs</h4>
				<br>
				<p class="authors">
					<medium><a style="font-weight: bold;" href="http://edouardleurent.com">Edouard Leurent</a>, </medium><br>
					<medium><a href="http://researchers.lille.inria.fr/~efimov/">Denis Efimov</a>, </medium><br>
					<medium><a href="http://odalricambrymmaillard.neowordpress.fr/">Odalric-Ambrym Maillard</a></medium>
				</p>
				<footer>
					<img data-src="images/univ.png" height="50">
					<img data-src="images/inria.png" height="50">
					<img data-src="images/cnrs.png" height="50">
					<img data-src="images/centrale.png" height="50">
					<img data-src="images/cristal.svg" height="50">
					<img data-src="images/renault.png" height="50">
				</footer>
				<aside class="notes">
				Hi everyone, my name is Edouard, and I will be presenting our joint work with Denis and Odalric about the robust adaptive control of linear systems, beyond quadratic costs.</aside>
			</section>

			<section>
				<header><h3>Motivation</h3></header>
				<p align="left">Most RL algorithms rely on trial and <span class="red">error</span>
					<ul>
						<li class="fragment"><span class="blue">Random</span> exploration</li>
						<li class="fragment"><span class="green">Optimism</span> in the face of uncertainty</li>
					</ul>
				</p>
				<br>
				<p  align="left" class="fragment">None are suitable for a <span class="blue">safety-critical</span> application
					<ul>
						<li class="fragment"><span class="red">Pessimism</span> in the face of uncertainty</li>
						<li class="fragment"><span class="green">Learn</span> from observations to improve performance</li>
					</ul>
				</p>
				<aside class="notes">
					Our motivation comes from the observation that most Reinforcement Learning algorithms rely on trial and error, through either ğŸ“ random exploration, ğŸ“ or optimism in the face of uncertainty<br>ğŸ“However, we argue that none of these two approaches are suitable for a safety-critical application, where failures are costly and must be avoided at all times.<br>ğŸ“In this work, we wish on the contrary to be pessimistic in the face of uncertainty in order to ensure safety, ğŸ“but we still want to be able to learn from our observations to reduce uncertainty and improve performance as time goes by.
				</aside>
			</section>

			<section>
				<header><h3>Setting</h3></header>
				<p>Linear dynamics with structured uncertainty<br><br>
					$
					\dot{{x}}(t)  = \color{orange}{A(\theta)}x(t) + Bu(t) + \omega(t),
					$
					<span class="fragment">
						$
						\quad\text{ where }\color{orange}{A(\theta)}  = A + \sum_{i=1}^d\color{orange}{\theta_i}\phi_i,
						$
						<br>
						<br>
						$A,\phi$ are <strong>known</strong>, and the disturbance $\omega(t)$ is <strong>bounded</strong>.
					</span>
				</p>
				<aside class="notes">
					Our setting is the following: we consider a linear system with state x, action u, bounded disturbance \omega, ğŸ“ and in which the dynamics linearly depend on a parameter \theta that is unknown.
				</aside>
			</section>

			<section>
				<header><h3>Robust control framework</h3></header>
				<ol>
					<li class="fragment"> Build a confidence <strong>region</strong> for the dynamics
						\[\mathbb{P}\left[\color{orange}{\theta}\in\color{crimson}{\mathcal{C}_{N,\delta}}\right]\geq 1-\delta\]
					</li>
					<li class="fragment"> Plan <strong>robustly</strong> against the worst case outcome $\color{crimson}{V^r}$
						\[\color{limegreen}{\sup_{u}} \underbrace{\color{crimson}{\inf_{\substack{\theta \in \mathcal{C}_{N,\delta}\\ \omega\in[\underline\omega,\overline\omega]}}} \expectedvalue \left[\sum_{n=0}^\infty \gamma^n R(x(t_n)) \middle| \color{limegreen}{u}, \color{crimson}{{\theta}}, \color{crimson}{\omega}\right]}_{\color{crimson}{V^r(u)}}\]
					</li>
				</ol>
				<aside class="notes">
					In order to ensure safety, we follow the robust control framework.<br>
					ğŸ“First, rather than a mere point estimate of the dynamics, we build an entire confidence region that contains the true parameter with high probability.<br>
					ğŸ“Second, we plan robustly wrt this uncertainty, that is, we maximise the worst-case performance for all admissible dynamical parameters \theta and disturbances \omega. This worst-case return is called the robust value V^r.
				</aside>
			</section>
			<section>
				<h3>Related work</h3>
				<ul>
					<li class="fragment">Robust Dynamic Programming <span class="fragment red">â®• finite $\mathcal{S}$</span></li>
					<li class="fragment">Quadratic costs (LQ) <span class="fragment red">â®• stabilization only</span></li>
				</ul>
				<br>
				<br>
				<p class="fragment">We only require the rewards $R$ to be <strong>bounded</strong>.</p>
				<aside class="notes">
					There are several lines of work in the literature attempting to solve this minimax control problem. They include<br>
					ğŸ“Robust dynamic programming techniques, ğŸ“but these are only applicable for finite state spaces<br>
					ğŸ“and the LQ setting where the costs are assumed to be quadratic, ğŸ“which limits applications to stabilization problems only.<br>
					ğŸ“In this work, we relax this assumption and only require the reward function to be bounded. In particular, we handle non-convex and even discontinuous reward functions, which allows to tackle a wider variety of problems.
				</aside>
			</section>
			<section><h3>Algorithm</h3>
				<aside class="notes">
				Our Algorithm is composed of 3 steps.</aside>
			</section>
			<section style="font-size: 80%">
				<header><h3>1. Model Estimation</h3></header>
				<div class="container">
					<div style="flex: 2;">
						<p class="fragment" data-fragment-index="0">Confidence ellipsoid $\color{crimson}{\cC_{N,\delta}}$ from<br><a href="https://papers.nips.cc/paper/4417-improved-algorithms-for-linear-stochastic-bandits">(Abbasi-Yadkori et al., 2011)</a><!-- non-asymptotic linear regression -->
							\[\small \mathbb{P}\left[\|\color{orange}{\theta}-\color{crimson}{\theta_{N}}\|_{\color{crimson}{G_{N}}} \leq \color{crimson}{\beta_N(\delta)}\right]\geq 1-\delta\]
							<!-- $
							\scriptsize
							\begin{aligned}
							\theta_{N,\lambda} & = G_{N, \lambda}^{-1} \sum_{n=1}^N \Phi_n^\top \Sigma_p^{-1} y_n,\\
							G_{N, \lambda} &= \sum_{n=1}^N \Phi_{n}^\top\Sigma_p^{-1}\Phi_{n}  + \lambda I_d
							\end{aligned}
							$ -->
						</p>
					</div>
					<div class="col" style="align-items: center;align-self: center;">
						<div class="r-stack">
							<img class="fragment fade-in-then-out" data-fragment-index="0" data-src="images/ellipsoid_0.svg" width=100%>
							<img class="fragment fade-in-then-out" data-fragment-index="1" data-src="images/ellipsoid_1.svg" width=100%>
							<img class="fragment fade-in-then-out" data-fragment-index="2" data-src="images/ellipsoid_2.svg" width=100%>
							<img class="fragment fade-in-then-out" data-fragment-index="3" data-src="images/ellipsoid_3.svg" width=100%>
						</div>
					</div>
				</div>
				<aside class="notes">
					First, ğŸ“we leverage the literature of non-asymptotic linear regression and adapt a result by Abbasi-Yadkori et al from 2011 to build a confidence ellipsoid for the dynamical parameters \theta. This ellipsoid shinks as we collect more and more datağŸ“ğŸ“ğŸ“.
				</aside>
			</section>
			<section>
				<header><h3>2. Interval Prediction</h3></header>
				<div class="container">
					<div style="flex: 2;">
						<p style="font-size: 80%">
							<span class="fragment" data-fragment-index="0">
							Propagate uncertainty $\color{crimson}{\theta\in\cC_{N,\delta}}$ through time and bound the reachable states
							</span>
							<span class="fragment" data-fragment-index="4">
							\[\color{lightskyblue}{\underline x(t)} \leq x(t) \leq \color{lightskyblue}{\overline x(t)}\]
							</span>
						</p>
					</div>
					<div class="col r-stack">
						<img class="fragment"  data-fragment-index="3" data-src="images/interval-hull_3.svg" width=100%>
						<img class="fragment"  data-fragment-index="0" data-src="images/interval-hull_0.svg" width=100%>
						<img class="fragment"  data-fragment-index="1" data-src="images/interval-hull_1.svg" width=100%>
						<img class="fragment"  data-fragment-index="2" data-src="images/interval-hull_2.svg" width=100%>
						<img class="fragment"  data-fragment-index="4" data-src="images/interval-hull_4.svg" width=100%>
					</div>
				</div>
				<p style="font-size: 0.7em" align="left" class="fragment" data-fragment-index="5">
					<a href="https://arxiv.org/abs/1904.04727">(Leurent et al., 2019)</a>
					\[
					\scriptsize
					\begin{aligned}
					\color{lightskyblue}{\dot{\underline{x}}(t)} & = \color{crimson}{A(\theta_N)}\color{lightskyblue}{\underline{x}(t)}-\color{crimson}{\Delta A_{+}}\underline{x}^{-}(t)-\color{crimson}{\Delta A_{-}}\overline{x}^{+}(t) +Bu(t)+D^{+}\underline{\omega}(t)-D^{-}\overline{\omega}(t),\\
					\color{lightskyblue}{\dot{\overline{x}}(t)} & = \color{crimson}{A(\theta_N)}\color{lightskyblue}{\overline{x}(t)}+\color{crimson}{\Delta A_{+}}\overline{x}^{+}(t)+\color{crimson}{\Delta A_{-}}\underline{x}^{-}(t) +Bu(t)+D^{+}\overline{\omega}(t)-D^{-}\underline{\omega}(t), \\
					\end{aligned}
					\]
				</p>
				<aside class="notes">
					Second, ğŸ“we propagate this model uncertainty ğŸ“throughğŸ“ timeğŸ“ to bound the set of reachable statesğŸ“ within an interval ğŸ“, denoted lower-bar x,  and upper-bar x, computed with a set-predictor that we introduced in prior work.
				</aside>
			</section>
			<section>
				<header><h3>3. Pessimistic planning</h3></header>
				<div class="container">
					<div style="flex: 2;">
						<p>Use the predicted <strong>intervals</strong> in a pessimistic <span class="orange">surrogate</span> objective
							<span>
								\[\small
								\color{orange}{\hat{V}^r(u)} = \sum_{n=N+1}^\infty \gamma^n \color{orange}{\min_{\color{lightskyblue}{\underline{x}(t_n)}\leq x \leq\color{lightskyblue}{\overline{x}(t_n)}}  R(x)}
								\]
							</span>
						</p>
					</div>
					<div class="col r-stack">
						<img class="fragment" data-src="images/planning.svg" width=100%>
					</div>
				</div>
				<aside class="notes">
					Third, we leverage these state intervals to build a pessimistic surrogate objective, which evaluates at each time-step the minimum of the reward function over the predicted interval.<br>
					ğŸ“This surrogate objective is easy to evaluate, and thus can be optimised by traditional techniques such as Monte-Carlo Tree Search.
				</aside>
			</section>
			<section>
				<header><h3>Results</h3></header>
				<div class="theorem fragment"><p><strong>Theorem (Lower bound)</strong></p>
					<p>
						$
						\color{orange}{\underbrace{\hat{V}^r(u)}_{\substack{\text{surrogate}\\\text{value}}}}
						\leq
						\color{crimson}{\underbrace{{V}^r(u)}_{\substack{\text{robust}\\\text{value}}}}$
						<span class="fragment">
							$\leq
							\color{limegreen}{\underbrace{{V}(u)}_{\substack{\text{true}\\\text{performance}}}}$
						</span>
					</p>
				</div>
				<aside class="notes">
					ğŸ“As a first result, we prove that the surrogate objective that we optimize is a lower-bound of the robust value, and that is because we are conservative in every approximation that we make.<br>
					ğŸ“The robust value is in turn a lower-bound of the true performance, by virtue of being pessimistic.
					However because of our approximations, there exists a gap between the surrogate and the robust values, which can cause suboptimal behaviour.
				</aside>
			</section>
			<section style="font-size: .7em">
				<header><h3>Bounded suboptimality</h3></header>
				<div class="theorem">
					<p align="left"><strong>Theorem</strong> Under two conditions:
						<ol>
							<li class="fragment">Lipschitz reward $R$;</li>
							<li class="fragment">Stability condition: there exist $P>0,\,Q_0,\,\rho,\,N_0$ such that
								\[\forall \color{orange}{N}>N_0,\quad\begin{bmatrix}
								\color{orange}{A({\theta}_{N})}^\top P + P \color{orange}{A({\theta}_{N})} + Q_0 & P|D|  \\
								|D|^\top P & -\rho I_r \\
								\end{bmatrix}< 0;\]
							</li>
						</ol>
					</p>
					<p align="left" class="fragment">
						with probability $1-\delta$,
						\[
						\underbrace{V(a_\star) - V(a_K)}_{\substack{\text{suboptimality}}} \leq
						\color{crimson}{\underbrace{\Delta_\omega}_{\substack{\text{robustness to}\\ \text{disturbances}}}} + \color{lightskyblue}{\underbrace{\mathcal{O}\left(\frac{\beta_N(\delta)^2}{\lambda_{\min}(G_{N,\lambda})}\right)}_{\text{estimation error}}} +
						\color{limegreen}{\underbrace{\mathcal{O}\left(K^{-\frac{\log 1/\gamma}{\log \kappa}}\right)}_{\text{planning error}}}
						\]
					</p>
				</div>
				<aside class="notes">
					In our second result, we bound this gap and resulting suboptimality under two conditions: namely, ğŸ“a smoothness assumption for the reward, ğŸ“and a stability condition for the estimated dynamics, in the form of a Linear Matrix Inequality.<br>
					ğŸ“Then, the suboptimality is bounded by three term: there is a constant which comes from the robustness to instantaneous disturbances, an estimation error term related to the size of the confidence ellipsoid, and a planning error term, which comes from the fact that we are optimizing with a finite computational budget.<br><br>
					A limitation of this result is that the validity of the stability condition nÂ°2. is difficult to check because it applies to matrices that are produced by our algorithm, rather than to the system parameters. Moreover, the estimation error term in the bound is input-dependent, and since we are being pessimistic, we are not actively exploring, so this term is not guaranteed to converge to 0 in general.
				</aside>
			</section>
			<section style="font-size: .7em">
				<header><h3>Asymptotic Near-optimality</h3></header>
				<div class="theorem">
					<p align="left"><strong>Corollary</strong> Under an additional <strong>persistent excitation</strong> (PE) assumption:
						\[\exists \underline{\phi},\overline{\phi}>0: \forall n\geq n_0,\quad \underline{\phi}^2 \leq \lambda_{\min}(\Phi_{n}^\top\Sigma_{p}^{-1}\Phi_{n}) \leq \overline{\phi}^2,\]
						<span class="fragment" data-fragment-index="1">
							the stability condition 2. can be relaxed to:
							$$\begin{bmatrix}
							\color{orange}{A(\theta)}^\top P + P \color{orange}{A(\theta)} + Q_0 & P|D|  \\
							|D|^\top P & -\rho I_r \\
							\end{bmatrix}< 0;$$
						</span>
						<span class="fragment" data-fragment-index="2">
							and the bound takes the more explicit form
						</span>
					</p>
					<p class="fragment" data-fragment-index="2">
						${V(a_\star)} - {V(a_K)} \leq
						\color{crimson}{\Delta_\omega} +$
						<span class="fragment shrink tiny" data-fragment-index="3">$\color{lightskyblue}{{\mathcal{O}\left(\frac{\log\left(N^{d/2}/\delta\right)}{N}\right)}}$</span>$ + $
						<span class="fragment shrink tiny" data-fragment-index="4">$\color{limegreen}{{\mathcal{O}\left(K^{-\frac{\log 1/\gamma}{\log \kappa}}\right)}}$</span>
					</p>
				</div>
				<aside class="notes">
					These concerns are partially adressed in a corollary, which states that under the favorable event that the features are sufficiently excited, ğŸ“ we can relax the stability condition to apply to the true system rather than our estimates, ğŸ“and the estimation error term in the bound takes a more explicit form which ensures asymptotic near optimality as the number of samples NğŸ“ and the planning budget KğŸ“ go to infinity.
				</aside>
			</section>
			<section><h3>Experiments</h3>
				<aside class="notes">
				We demonstrate the applicability of our method in two experiments.</aside>
			</section>
			<!-- <section>
				<header><h3>Experiment: obstacle avoidance</h3></header>
				<figure>
					<video class="js-play-toggle" muted loop data-src="../../supplementary/videos/1_obstacle/1_oracle.mp4" type="video/mp4"></video>
					<figcaption>Oracle dynamics $\color{orange}{A(\theta)}$</figcaption>
				</figure>
			</section> -->
			<section>
				<header><h3>Obstacle avoidance</h3></header>
				<figure>
					<video class="js-play-toggle" muted loop data-src="../../supplementary/videos/1_obstacle/1_plan_nominal.mp4" type="video/mp4"></video>
					<figcaption>Estimated dynamics $\color{orange}{A({\theta}_{N})}$</figcaption>
				</figure>
				<aside class="notes">
					First, we consider an obstacle avoidance problem with unknown friction coefficients. When planning with the estimated parameters \theta_N, the model bias leads to prediction errors, and eventually to collisions.
				</aside>
			</section>
			<section>
				<header><h3>Obstacle avoidance</h3></header>
				<figure>
					<video class="js-play-toggle" muted loop data-src="../../supplementary/videos/1_obstacle/1_robust.mp4" type="video/mp4"></video>
					<figcaption>Robust planning with $\color{crimson}{\cC_{N,\delta}}$</figcaption>
				</figure>
				<aside class="notes">
					In contrast, when planning robustly with respect to model uncertainty, our confidence intervals over trajectories ensure that we never get too close to an obstacle and reach the goal safely.
					Here, the estimated confidence ellipsoid is shown on the right panel.
				</aside>
			</section>
			<section>
				<header><h3>Obstacle avoidance (2)</h3></header>
				<figure>
					<video class="js-play-toggle" muted loop data-src="../../supplementary/videos/1_obstacle/2_plan_nominal.mp4" type="video/mp4"></video>
					<figcaption>Estimated dynamics $\color{orange}{A({\theta}_{N})}$</figcaption>
				</figure>
				<aside class="notes">
					Here is another example, where the nominal agent collides with an obstacle
				</aside>
			</section>
			<section>
				<header><h3>Obstacle avoidance (2)</h3></header>
				<figure>
					<video class="js-play-toggle" muted loop data-src="../../supplementary/videos/1_obstacle/2_robust.mp4" type="video/mp4"></video>
					<figcaption>Robust planning with $\color{crimson}{\cC_{N,\delta}}$</figcaption>
				</figure>
				<aside class="notes">
					and the robust agent in the same scene.
				</aside>
			</section>
			<section>
				<header><h3>Results</h3></header>
				<table>
					<thead>
						<tr>
							<th>Performance</th>
							<th>failures</th>
							<th>min</th>
							<th>avg $\pm$ std</th>
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>Oracle</td>
							<td>$0\%$</td>
							<td>$11.6$</td>
							<td>$14.2 \pm 1.3$</td>
						</tr>
						<tr>
							<td>Nominal</td>
							<td>$4\%$</td>
							<td>$2.8$</td>
							<td><span class="fragment highlight-blue" data-fragment-index="1">$13.8 \pm 2.0$</span></td>
						</tr>
						<tr>
							<td>DQN (trained)</td>
							<td>$6\%$</td>
							<td>$1.7$</td>
							<td>$12.3 \pm 2.5$</td>
						</tr>
						<tr>
							<td>Robust</td>
							<td><span class="fragment highlight-blue" data-fragment-index="0">$0\%$</span></td>
							<td><span class="fragment highlight-blue" data-fragment-index="0">$10.4$</span></td>
							<td>$13.0 \pm 1.5$</td>
						</tr>
					</tbody>
				</table>
				<aside class="notes">
					Over 100 runs, our proposed robust agent ğŸ“manages to avoid collisions and achieves the highest minimum return, ğŸ“at the price of a loss of average performance compared to the nominal approach that plans with the estimated dynamics.
				</aside>
			</section>
			<section>
				<header><h3>Empirical suboptimality</h3></header>
				<img data-src="images/regret.svg" width="70%">
				<aside class="notes">
					In this figure, we plot the average suboptimality for each agent, along with its associated 95% confidence interval, with respect to the number N of observed transitions. The suboptimality is evaluated as follows: in each state of a trajectory, we substract the empirical return achieved by the agent from the optimal return that it would have obtained had it acted optimally, which is computed by running a oracle planner with access to the true dynamics and a high computational budget in parallel.

					This graph shows that, despite the fact that the assumptions of our Theorem do not hold, since for example the rewards are discontinuous at collision states, the suboptimality of the robust agent still decreases with the number of observations. Thus, as we desired, the agent gets more efficient as it is more confident, while acting safely at all times.
					The nominal agent performs better on average, but has a higher variance and suffers collisions from time to time.
				</aside>
			</section>
			<section style="font-size: 90%;">
				<header><h3>Multi-model extension</h3></header>
				<div class="container">
					<div class="col">
						<p>What is our modelling assumption<br> $\color{orange}{A(\theta)} = \color{lightskyblue}{A} + \sum_{i=1}^d\color{orange}{\theta_i}\color{lightskyblue}{\phi_i}$ is <span class="red">wrong</span>?
							<ul>
								<li class="fragment idea" data-fragment-index="0">The <strong>adequacy</strong> of a model $\color{lightskyblue}{(A,\phi)}$ can be evaluated</li>
								<li class="fragment imply" data-fragment-index="1">Use <strong>multiple</strong> models $\color{lightskyblue}{(A^m,\phi^m)}$, <br>through a robust selection mechanism</li>
							</ul>
						<figure><img class="fragment" data-fragment-index="1" width="400" data-src="images/robust/multi-model.svg">
<!-- 							<figcaption><em>Multimodal predictions</em></figcaption>
 -->						</figure>
					</div>
				</div>
				<aside class="notes">
					Our algorithm has been relying on a modelling assumption about the structure of the state matrix. But what if it does not hold?
					ğŸ“A benefit of our approach is that the adequacy of a model with observations can be evaluated, by assessing the validity of the predicted confidence intervals whenever a new state transition is observed.
					ğŸ“This allows us to propose an extension of our framework in which multiple modelling assumptions are considered, rejected and balanced through a robust selection mechanism. This provides the robust agent with the ability to make multimodal predictions.
				</aside>
			</section>
			<section>
				<header><h3>Driving</h3></header>
				<figure>
					<video class="js-play-toggle" muted loop data-src="../../supplementary/videos/2_driving/1_robust.mp4" type="video/mp4" width="50%"></video>
					<figcaption><small><img data-src="images/octocat.png" width="32px" style="margin:0"><a href="http://www.github.com/eleurent/highway-env/">github.com/eleurent/highway-env/</a></small></figcaption>
				</figure>
				<aside class="notes">
					And we illustrate this ability in a second experiment, where an autonomous vehicle must cross an unsignalized intersection while driving among other vehicles whose destinations and driving styles are unknown.
				</aside>
			</section>
			<section>
				<header><h3>Driving (2)</h3></header>
				<figure>
					<video class="js-play-toggle" muted loop data-src="../../supplementary/videos/2_driving/4_robust.mp4" type="video/mp4" width="50%"></video>
					<figcaption><small><img data-src="images/octocat.png" width="32px" style="margin:0"><a href="http://www.github.com/eleurent/highway-env/">github.com/eleurent/highway-env/</a></small></figcaption>
				</figure>
				<aside class="notes">
					When the number of vehicles and resulting uncertainty are high, the robust agent is fairly careful and waits until the intersection is clear before continuing.
				</aside>
			</section>
			<section>
				<header><h3>Driving (3)</h3></header>
				<figure>
					<video class="js-play-toggle" muted loop data-src="../../supplementary/videos/2_driving/9_robust.mp4" type="video/mp4" width="50%"></video>
				<figcaption><small><img data-src="images/octocat.png" width="32px" style="margin:0"><a href="http://www.github.com/eleurent/highway-env/">github.com/eleurent/highway-env/</a></small></figcaption>
				</figure>
				<aside class="notes">
					Here is another episode, where the agent confidently identifies a sufficient gap, and successfully merges into traffic.
				</aside>
			</section>
			<section>
				<header><h3>Results</h3></header>
				<table>
					<thead>
						<tr>
							<th>Performance</th>
							<th>failures</th>
							<th>min</th>
							<th>avg $\pm$ std</th>
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>Oracle</td>
							<td>$0\%$</td>
							<td>$6.9$</td>
							<td>$7.4 \pm 0.5$</td>
						</tr>
						<tr>
							<td>Nominal 1</td>
							<td>$4\%$</td>
							<td>$5.2$</td>
							<td><strong>$7.3 \pm 1.5$</strong></td>
						</tr>
						<tr>
							<td>Nominal 2</td>
							<td>$33\%$</td>
							<td>$3.5$</td>
							<td>$6.4 \pm 0.3$</td>
						</tr>
						<tr>
							<td>DQN (trained)</td>
							<td>$3\%$</td>
							<td>$5.4$</td>
							<td>$6.3 \pm 0.6$</td>
						</tr>
						<tr>
							<td>Robust</td>
							<td><strong>$0\%$</strong></td>
							<td><strong>$6.8$</strong></td>
							<td>$7.1 \pm 0.3$</td>
						</tr>
					</tbody>
				</table>
				<aside class="notes">
					As before, over 100 runs the robust agent manages to ensure safety at all times contrary to other baselines, at the price of a decreased average performance.
				</aside>
			</section>
			<section>
				<header><h2>Thank You!</h2></header>
				<br>
				<p>Code available at<br><small><img data-src="images/octocat.png" width="32px" style="margin:0"><a href="http://eleurent.github.io/robust-beyond-quadratic/">eleurent.github.io/robust-beyond-quadratic</a></small></p>
				<p><em>I am looking for a postdoctoral position.</em></p>
				<aside class="notes">
					So, this is it, the source code of our experiments is available at the following URL, and I would like to take this opportunity to mention that I am looking for a postdoctoral position.
					Thank you for your attention!
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/search/search.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script async defer src="https://buttons.github.io/buttons.js"></script>
		<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			autoPlayMedia: true,
			hash: true,
			math: {
				mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				config: 'TeX-AMS_HTML-full',
		      // pass other options into `MathJax.Hub.Config()`
		      TeX: { Macros: {
		      	Real: "{\\mathbb{R}}",
		      	cC: "{\\mathcal{C}}",
		      	expectedvalue: "\\mathop{\\mathbb{E}}",
		      }}
		  },
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [ RevealSearch, RevealHighlight, RevealNotes, RevealMath, RevealZoom ]
		});

		Reveal.addEventListener( 'slidechanged', function( e ) {
			e.currentSlide.querySelectorAll('.js-play-toggle').forEach(v => v.play());
		});
	</script>
</body>
</html>
